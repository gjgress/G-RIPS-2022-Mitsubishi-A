{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2499d1-d466-47c7-bfba-e0605416a0f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Map Matching Benchmarks\n",
    "\n",
    "This notebook is intended as a supplement to the Sendai Map notebook. This notebook implements the competing map matching algorithms and tests them against the map-matching-dataset for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6a375a-1580-42e2-8906-e25c32d3a587",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjgress/.local/lib/python3.10/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox \n",
    "import time\n",
    "from shapely.geometry import Polygon, Point\n",
    "import os\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from algorithms import mm_utils\n",
    "import dask\n",
    "import dask.bag as db\n",
    "from functools import reduce\n",
    "from scipy.optimize import linprog\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Remove this when debugging\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import dask.bag as db\n",
    "\n",
    "%matplotlib inline\n",
    "#ox.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce2a800-3ab1-4794-ae6f-041e7807bb44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Input\n",
    "\n",
    "df_track = db.read_text('map-matching-dataset/*track.geojson').map(json.loads).map(gpd.GeoDataFrame.from_features)\n",
    "df_network_edges = db.read_text('map-matching-dataset/*arcs.geojson').map(json.loads).map(gpd.GeoDataFrame.from_features)\n",
    "df_network_nodes = db.read_text('map-matching-dataset/*nodes.geojson').map(json.loads).map(gpd.GeoDataFrame.from_features)\n",
    "df_gt = db.read_text('map-matching-dataset/*route.geojson').map(json.loads).map(gpd.GeoDataFrame.from_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ef7f7-b018-45ba-804c-71cf9b73da55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In our case, our data is already fused. But often you will have several datasets with asynchronous data that you will have to fuse first. We implemented a barebones method in mm_utils to handle this; you can see an example of how to use it in '02a Data Pre-Processing'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d1760-9488-4c9c-9122-9454d3eef862",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Typically GPS/IMU data is recorded as Point geometries in a GDF.\n",
    "However, some algorithms require a trajectory (LineStrings) despite this.\n",
    "As a result, our framework requires both points (nodes) and trajectories (edges).\n",
    "So we will need to create a \"trajectory\" by sequentially connecting our nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7172f1bc-ea8a-4ed7-b457-b3a8347252b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I wrote a utility function to do that, provided in mm_utils.\n",
    "\n",
    "df_track_edges = df_track.map(mm_utils.point_to_traj)#, columns = {'timestamp':'first',\n",
    "                                                     #  'altitude':'average',\n",
    "                                                     #  'speed':'average',\n",
    "                                                     #  'vertical accuracy':'last',\n",
    "                                                     #  'horizontal accuracy':'last',\n",
    "                                                     #  'oops':'notavalidmethod'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd175119-eb77-4056-be8c-587a0f92c181",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "There may be other data assigned to the nodes which we would like the edges to also reflect.\n",
    "Some algorithms may use the auxiliary data from edges, which is why this is a concern.\n",
    "There's no perfect way to do this assignment, but I included a few basic methods in mm_utils: 'first' node assignment, 'average' between nodes, and 'last' node assignment. That is the columns argument I used above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef35e6-a9c2-4873-b04c-4402c40e59ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we demonstrate how we can work with several algorithms at once in a modular fashion.\n",
    "\n",
    "First we initialize the simulators, to be later applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a715941-594b-4d0b-8b18-38cc78affd40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from algorithms import metric_mm#, fmm_bin \n",
    "#from fmm import FastMapMatchConfig\n",
    "\n",
    "### Define map matching configurations\n",
    "\n",
    "k = 8\n",
    "radius = 0.003\n",
    "gps_error = 0.0005\n",
    "\n",
    "#fmm_config = FastMapMatchConfig(k,radius,gps_error)\n",
    "cfg_file = None\n",
    "\n",
    "## Least squares functions\n",
    "ls_ri = lambda distarray: np.square(distarray) # The function applied directly to the distances from the candidate route to the k-NN GPS coords\n",
    "ls_ro = lambda distarray: 1*(1/np.size(distarray) * np.sum(distarray)) # This is where we 'integrate' over the distances, and if we need to do anything else, we do it\n",
    "ls_gi = lambda distarray: np.square(distarray) # The function applied directly to the distances from the GPS coords to the k-NN candidate route nodes\n",
    "ls_go = lambda distarray: 1*(1/np.size(distarray) * np.sum(distarray))\n",
    "##\n",
    "\n",
    "## Inverse squares function ('Electrical method')\n",
    "eps = 0.0000001\n",
    "is_ri = lambda distarray: np.power(np.square(distarray) + eps, -1) # We need eps to prevent singularities, i.e. r = 0\n",
    "is_ro = lambda distarray: -1*(1/np.size(distarray) * np.sum(distarray)) # We sum, and then multiply by -1 to turn the minimizing process into a maximizing process\n",
    "is_gi = lambda distarray: np.power(np.square(distarray) + eps, -1)\n",
    "is_go = lambda distarray: -1*(1/np.size(distarray) * np.sum(distarray))\n",
    "##\n",
    "\n",
    "def wrapper_f(ri, ro, gi, go): # This should return a function composed from the basic functions, that can then be applied onto route and gps data.\n",
    "    return lambda route, gps : 1*ro(ri(route)) + 1*go(gi(gps))\n",
    "\n",
    "ls_loss_function = wrapper_f(ls_ri, ls_ro, ls_gi, ls_go)\n",
    "is_loss_function = wrapper_f(is_ri, is_ro, is_gi, is_go)\n",
    "\n",
    "def wasserstein(routeloss, gpsloss):#gpsloss,n,m\n",
    "    #the (i,j)th entry of the gpsloss matrix is the distance from the ith point of the trajectory to the jth point on the candiate route\n",
    "    # n is the number of points along the trajectory\n",
    "    # m is the number of points on the candidate route\n",
    "    n = gpsloss.shape[0]\n",
    "    m = gpsloss.shape[1]\n",
    "    #the (i,j)th entry of the gpsloss matrix is the distance from the ith point of the trajectory to the jth point on the candiate route\n",
    "    #Create equality constraints\n",
    "    b = [1/n for i in range(0,n)]+ [1/m for i in range(0,m)]\n",
    "    row1 = [i for i in range(0,n) for j in range(0,m)]\n",
    "    row2 = [n+j for j in range (0,m) for i in range(0,n)]\n",
    "    row = np.append(np.matrix.flatten(np.array(row1)),np.matrix.flatten(np.array(row2)))\n",
    "    col1 = [list(range(0,n*m))]\n",
    "    col2 =  [j+m*k for j in range(0,m) for k in range(0,n)]\n",
    "    col = np.append(np.matrix.flatten(np.array(col1)),np.matrix.flatten(np.array(col2)))\n",
    "    data = np.ones(n*m*2)\n",
    "    A = csr_matrix((data, (row, col)),shape = (n+m, n*m)).toarray()\n",
    "    A = A[:-1]\n",
    "    b = b[:-1]\n",
    "    #solve the linear program\n",
    "    res = linprog(np.matrix.flatten(gpsloss),None, None,A,b)\n",
    "    #return the function value, i.e. the wasserstein distance\n",
    "    loss = res.fun\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28839dab-5018-4ecb-9888-3c094561ff0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The metric_mm algorithm allows you to either directly pass in a loss function, or to pass it all the individual pieces and wrap it itself\n",
    "# This may be useful in case you wish to utilize the individual functions of a sim elsewhere.\n",
    "\n",
    "#sim1 = fmm_bin.FMM(cfg = fmm_config)\n",
    "sim2 = metric_mm.Sim(ls_ri, ls_ro, ls_gi, ls_go, wrapper_f) # Least squares metric-based\n",
    "sim3 = metric_mm.Sim(loss_function = is_loss_function)\n",
    "#sim4 = metric_mm.Sim(loss_function = wasserstein)\n",
    "\n",
    "\n",
    "## If you have the ground truth, load it here\n",
    "ground_truth = db.read_text('map-matching-dataset/*route.geojson').map(json.loads).map(gpd.GeoDataFrame.from_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3700509-e0bd-4622-8ead-922ca0c3960b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## We will convert our Dask Bags to Dask Delayed objects so we can iterate over them. Then we will lazily call our function on our dataset\n",
    "# Finally we will compute our results, and Dask will automatically parallelize our work.\n",
    "\n",
    "gt = ground_truth.to_delayed()\n",
    "te = df_track_edges.to_delayed()\n",
    "tn = df_track.to_delayed()\n",
    "ne = df_network_edges.to_delayed()\n",
    "nn = df_network_nodes.to_delayed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68294ee-1ae1-42dd-9f0b-9bdca813e19b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5h 18min 43s, sys: 17 s, total: 5h 19min\n",
      "Wall time: 7h 44min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We will precompute candidate routes for the first ten datasets, and then save them. This will greatly speed up the runtime of the metric-based loss methods and the Wasserstein loss methods.\n",
    "\n",
    "all_cands = []\n",
    "\n",
    "n = 10\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    if not os.path.exists('map-matching-dataset/0000000' + str(i) + 'candroutes.geojson'):\n",
    "    \n",
    "        network_edges = ne[i].compute()[0]\n",
    "        track_nodes = tn[i].compute()[0]\n",
    "                \n",
    "        qry_pts = [y for sublist in [x.coords[:] for x in network_edges['geometry']] for y in sublist]            \n",
    "        source_index, _ = mm_utils.get_nearest([(track_nodes['geometry'].iloc[0].x, track_nodes['geometry'].iloc[0].y)], qry_pts, k_neighbors = 1)\n",
    "        source_index = source_index[0][0]\n",
    "        target_index, _ = mm_utils.get_nearest([(track_nodes['geometry'].iloc[-1].x, track_nodes['geometry'].iloc[-1].y)], qry_pts, k_neighbors = 1)\n",
    "        target_index = target_index[0][0]\n",
    "\n",
    "        source = Point(qry_pts[source_index])\n",
    "        target = Point(qry_pts[target_index])\n",
    "\n",
    "        candidates = mm_utils.get_nearest_edges(track_nodes, network_edges, k_neighbors=16, r = 0.1) \n",
    "        all_candidate_edges = reduce(lambda left,right: pd.concat([left, right]).drop_duplicates(subset=['geometry']), candidates)\n",
    "        candidate_routes = dask.delayed(mm_utils.dijkstra)(source, target, all_candidate_edges) # This is the bottleneck in this situation. I wish I had time to write an informed Dijkstra method\n",
    "        all_cands.append(candidate_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6ef2b-fad0-4488-861f-4aaca08c9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pray that your parameters are correct and Dijsktra's algorithm can find a route; otherwise you'll waste a lot of time when this cell fails\n",
    "#all_cands = dask.compute(*all_cands)\n",
    "\n",
    "# I wish this found valid routes for all of them, but for some reason it doesn't, so I can't actually utilize parallization here\n",
    "\n",
    "for i in range(n):\n",
    "    try:\n",
    "        all_cands[i] = dask.compute(all_cands[i])\n",
    "    except:\n",
    "        all_cands[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6190dfa-d701-4fa7-88fa-4289af592b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save them to file\n",
    "\n",
    "for i in range(len(all_cands)):\n",
    "    candidate_routes = all_cands[i][0]\n",
    "    j = i\n",
    "    while os.path.exists('map-matching-dataset/0000000' + str(j) + 'candroutes.geojson'):\n",
    "        j = j+1\n",
    "    with open('map-matching-dataset/0000000' + str(j) + 'candroutes.geojson', 'w') as f:\n",
    "        for j, item in enumerate(candidate_routes):\n",
    "            if j > 0:\n",
    "                f.write('\\n')\n",
    "            f.write('%s' % item.to_json())\n",
    "        # Note that each line of the GeoJSON file is valid GeoJSON, but the file as a whole is not valid GeoJSON.\n",
    "    with open('map-matching-dataset/0000000' + str(j) + 'candroutes.geojson', 'w') as f:\n",
    "        f.write('\\n')\n",
    "        # Create a blank file, so we won't try to run it again on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad3754-3fc9-424f-ac6a-aff56a670eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we want to reconstruct this list, we do the following:\n",
    "\n",
    "all_cands = []\n",
    "\n",
    "for i in range(n):\n",
    "    cand_routes = []\n",
    "    with open('map-matching-dataset/0000000'+ str(i) + 'candroutes.geojson', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            gjson = json.loads(line)\n",
    "            cand_routes.append(gpd.GeoDataFrame.from_features(gjson))\n",
    "    all_cands.append(cand_routes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec4583-0502-4ac8-bfce-3129cd9264d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we are ready to run the simulator on a subsection (or all of) the data. Notice how easy it is to run algorithms in parallel-- all of the parallelization is handled by Dask Delayed, so even though our algorithm is only designed to handle one case at a time, it has already gained magnitudes of efficiency.\n",
    "\n",
    "One small caveat-- even if your algorithm is technically 'independent', if you utilize python.os functions, you may run into I/O read/write errors. To circumvent this, use a dedicated library such as tempfile to systematically handle the temp file creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce797468-bde0-4a2b-a97c-9e27d3d5e3d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "## Let's see this!\n",
    "sim2_results = []\n",
    "sim3_results = []\n",
    "#sim4_results = []\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    ite = te[i].compute()[0]\n",
    "    itn = tn[i].compute()[0]\n",
    "    ine = ne[i].compute()[0]\n",
    "    inn = nn[i].compute()[0]\n",
    "    \n",
    "    # Load candidate routes\n",
    "    \n",
    "    cand_routes = []\n",
    "    with open('map-matching-dataset/0000000' + str(i) + 'candroutes.geojson', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            gjson = json.loads(line)\n",
    "            cand_routes.append(gpd.GeoDataFrame(gjson))\n",
    "#    %timeit sim1_results.append(sim1.run(ite, itn, ine, inn, return_results=True))\n",
    "    \n",
    "    sim2 = metric_mm.Sim(ls_ri, ls_ro, ls_gi, ls_go, wrapper_f)\n",
    "    sim2.preprocessing(ite, ine, candidate_routes = all_cands[i], n = 10, m = 1)\n",
    "    sim2_results.append(dask.delayed(sim2.run)(k1 = 10, k2 = 10, return_results = True, parallel = True))\n",
    "    \n",
    "    sim3 = metric_mm.Sim(loss_function = is_loss_function,\n",
    "                         trajectory = sim2.trajectory,\n",
    "                         candidate_route_nodes = sim2.candidate_route_nodes,\n",
    "                         candidate_routes = sim2.candidate_routes)    \n",
    "    sim3_results.append(dask.delayed(sim3.run)(k1 = 10, k2 = 10, return_results = True, parallel = True))\n",
    "        \n",
    "#    sim4 = metric_mm.Sim(loss_function = wasserstein)\n",
    "#    sim4.preprocessing(ite, ine, candidate_routes = all_cands[i], n = 1, m = 1)\n",
    "#    sim4_results.append(dask.delayed(sim2.run)(k1 = 1, k2 = -1, return_results = True, parallel = True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a5201-02d1-4e59-b562-831c1ee642f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit sim2_results = dask.compute(*sim2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10eaa32-85a3-4105-aa79-bdbdb2c361ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sim3_results = dask.compute(*sim3_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1282a2-7438-4b53-8a54-aed04bd801f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit sim4_results = dask.compute(*sim4_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8694de-546f-40ac-a420-aed96ad984d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we iterate through our results, and evaluate it using the build-in evaluation method in mm_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274f1a5-6725-4ec8-b4da-3fe6c5b314c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sim1_errors = []\n",
    "sim2_errors = []\n",
    "sim3_errors = []\n",
    "#sim4_errors = []\n",
    "\n",
    "for i in range(n):\n",
    "#    errors.append(dask.delayed(mm_utils.evaluate)(sim1_results[i],gt[i], matchid = \"index\")) # A more standard match method would be 'geometry', but in this case index is more reliable\n",
    "# For whatever reason, delayed isn't working here, so I'm just going to do this manually\n",
    "\n",
    "#    sim1_errors.append(mm_utils.evaluate(sim1_results[i],\n",
    "#                                    gt[i].compute()[0],\n",
    "#                                    matchid = \"index\")) # A more standard match method would be 'geometry', but in this case index is more reliable\n",
    "    sim2_errors.append(mm_utils.evaluate(sim2_results[i],\n",
    "                                    gt[i].compute()[0],\n",
    "                                    matchid = \"geometry\"))\n",
    "    sim3_errors.append(mm_utils.evaluate(sim3_results[i],\n",
    "                                    gt[i].compute()[0],\n",
    "                                    matchid = \"geometry\"))\n",
    "#    sim4_errors.append(mm_utils.evaluate(sim4_results[i],\n",
    "#                                    gt[i].compute()[0],\n",
    "#                                    matchid = \"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703e368-b73f-4486-85b8-f6957d1fc51e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(1)\n",
    "#plt.ylim(0, 1.2)\n",
    "# plt.boxplot(sim1_errors,vert=False)\n",
    "# plt.title('FMM Error %s on ' \n",
    "#           + str(n) \n",
    "#           + ' test cases \\n(Average error = ' \n",
    "#           + str(np.average(sim1_errors)) + ')')\n",
    "# plt.show()\n",
    "#plt.savefig(\"fmmerror.png\",bbox_inches='tight',dpi=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f17e01-a7e3-4ab6-8cdb-ca69c0930401",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "plt.ylim(0, 1.2)\n",
    "plt.boxplot(sim2_errors, figsize = (16,8), vert=False)\n",
    "plt.title('Least Squares Error %s on ' \n",
    "          + str(n) \n",
    "          + ' test cases \\n(Average error = ' \n",
    "          + str(np.average(sim2_errors)) + ')')\n",
    "plt.show()\n",
    "plt.savefig(\"leastsquareerror.png\",bbox_inches='tight',dpi=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b93275-456b-4b9f-b74c-14b0349d467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1)\n",
    "plt.ylim(0, 1.2)\n",
    "plt.boxplot(sim3_errors,vert=False)\n",
    "plt.title('Electrical Method Error %s on ' \n",
    "          + str(n) \n",
    "          + ' test cases \\n(Average error = ' \n",
    "          + str(np.average(sim3_errors)) + ')')\n",
    "plt.show()\n",
    "plt.savefig(\"electricerror.png\",bbox_inches='tight',dpi=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(1)\n",
    "#plt.ylim(0, 1.2)\n",
    "# plt.boxplot(sim4_errors,vert=False)\n",
    "# plt.title('Wasserstein Error %s on ' \n",
    "#           + str(n) \n",
    "#           + ' test cases \\n(Average error = ' \n",
    "#           + str(np.average(sim4_errors)) + ')')\n",
    "# plt.show()\n",
    "#plt.savefig(\"wassersteinerror.png\",bbox_inches='tight',dpi=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37561430-9bb6-4050-9462-1edb27639615",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Overall, FMM seems to handle itself okay, with the exception of one huge outlier!\n",
    "\n",
    "(Note-- having an error above 100% is not a bug; the standard error formula is not upper-bounded, despite being a percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56649bdd-313c-40d6-8393-c54390516672",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Let's see how they performed on one datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f79374-74f1-4fbe-a345-e4ec7b33596a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 4\n",
    "fig, axs = plt.subplots(2,n, figsize=(24,16))\n",
    "\n",
    "fig.suptitle('Evaluation Methods Visualized')\n",
    "\n",
    "# netemp = ne[0].compute()[0]\n",
    "# gttemp = gt[0].compute()[0]\n",
    "# netemp.plot(ax=axs[0,0])\n",
    "# tn[0].compute()[0].plot(ax=axs[0,0], color='black',markersize=2)\n",
    "# netemp.plot(ax=axs[1,0])\n",
    "# gttemp.plot(ax=axs[1,0], color='green')\n",
    "# netemp.plot(ax=axs[2,0])\n",
    "# sim1_results[0].plot(ax=axs[2,0], color= 'red')\n",
    "# evalint = gttemp.loc[np.intersect1d(gttemp['index'], sim1_results[0]['index'], return_indices=True)[1]]\n",
    "# evalxor = pd.concat([gttemp.overlay(evalint, how=\"difference\"), sim1_results[0].overlay(evalint, how = \"difference\")])\n",
    "# evalxor.plot(ax=axs[3,0], color='purple')\n",
    "# axs[3,0].text(0.5, -0.5, 'Error: ' + str(sim1_errors[0]), size=10, ha= 'center', transform=axs[3,0].transAxes)\n",
    "\n",
    "netemp = ne[0].compute()[0]\n",
    "gttemp = gt[0].compute()[0]\n",
    "netemp.plot(ax=axs[0,0])\n",
    "tn[0].compute()[0].plot(ax=axs[0,0], color='black',markersize=2)\n",
    "netemp.plot(ax=axs[1,0])\n",
    "gttemp.plot(ax=axs[1,0], color='green')\n",
    "netemp.plot(ax=axs[2,0])\n",
    "sim2_results[0].plot(ax=axs[2,0], color= 'red')\n",
    "evalint = gttemp.loc[np.intersect1d(gttemp['index'], sim2_results[0]['index'], return_indices=True)[1]]\n",
    "evalxor = pd.concat([gttemp.overlay(evalint, how=\"difference\"), sim2_results[0].overlay(evalint, how = \"difference\")])\n",
    "evalxor.plot(ax=axs[3,0], color='purple')\n",
    "axs[3,0].text(0.5, -0.5, 'Error: ' + str(sim2_errors[0]), size=10, ha= 'center', transform=axs[3,0].transAxes)\n",
    "\n",
    "netemp = ne[0].compute()[0]\n",
    "gttemp = gt[0].compute()[0]\n",
    "netemp.plot(ax=axs[0,1])\n",
    "tn[0].compute()[0].plot(ax=axs[0,1], color='black',markersize=2)\n",
    "netemp.plot(ax=axs[1,1])\n",
    "gttemp.plot(ax=axs[1,1], color='green')\n",
    "netemp.plot(ax=axs[2,1])\n",
    "sim3_results[0].plot(ax=axs[2,1], color= 'red')\n",
    "evalint = gttemp.loc[np.intersect1d(gttemp['index'], sim3_results[0]['index'], return_indices=True)[1]]\n",
    "evalxor = pd.concat([gttemp.overlay(evalint, how=\"difference\"), sim3_results[0].overlay(evalint, how = \"difference\")])\n",
    "evalxor.plot(ax=axs[3,1], color='purple')\n",
    "axs[3,1].text(0.5, -0.5, 'Error: ' + str(sim3_errors[0]), size=10, ha= 'center', transform=axs[3,1].transAxes)\n",
    "\n",
    "# netemp = ne[0].compute()[0]\n",
    "# gttemp = gt[0].compute()[0]\n",
    "# netemp.plot(ax=axs[0,2])\n",
    "# tn[0].compute()[0].plot(ax=axs[0,2], color='black',markersize=2)\n",
    "# netemp.plot(ax=axs[1,2])\n",
    "# gttemp.plot(ax=axs[1,2], color='green')\n",
    "# netemp.plot(ax=axs[2,2])\n",
    "# sim4_results[0].plot(ax=axs[2,2], color= 'red')\n",
    "# evalint = gttemp.loc[np.intersect1d(gttemp['index'], sim4_results[0]['index'], return_indices=True)[1]]\n",
    "# evalxor = pd.concat([gttemp.overlay(evalint, how=\"difference\"), sim3_results[0].overlay(evalint, how = \"difference\")])\n",
    "# evalxor.plot(ax=axs[3,2], color='purple')\n",
    "# axs[3,2].text(0.5, -0.5, 'Error: ' + str(sim4_errors[0]), size=10, ha= 'center', transform=axs[3,2].transAxes)\n",
    "\n",
    "\n",
    "fig.legend(handles=[axs[0,0].collections[0],axs[0,0].collections[1],axs[1,0].collections[1],axs[2,0].collections[1],axs[3,0].collections[0]], labels = ['Road Network', 'GPS Datapoints', 'Ground Truth', 'Prediction', 'Difference between GT and Pred.'], loc = 'lower left')\n",
    "plt.savefig(\"casestudy.png\",bbox_inches='tight',dpi=100) \n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
