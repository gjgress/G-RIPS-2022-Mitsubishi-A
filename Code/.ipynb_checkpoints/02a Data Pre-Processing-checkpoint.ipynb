{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2499d1-d466-47c7-bfba-e0605416a0f7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Is your data too messy to be utilized in 02? Look no further! This notebook walks through the data pre-processing methodology for our datasets, particularly BDD100K. We also include some helpful tips to make your data more compatible with these notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6a375a-1580-42e2-8906-e25c32d3a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjgress/.local/lib/python3.10/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox \n",
    "import time\n",
    "from shapely.geometry import Polygon\n",
    "import os, io, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from algorithms import mm_utils\n",
    "\n",
    "%matplotlib inline\n",
    "#ox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428da85e-a982-4073-ac53-7f3ea62ea441",
   "metadata": {},
   "source": [
    "# Importing your Data\n",
    "\n",
    "If you are seriously testing your algorithm against data, chances are your dataset is huge. Blindly trying to import it into a Pandas (Geo)DataFrame is going to cause some issues, because it will attempt to load it all into memory (which is likely impossible).\n",
    "\n",
    "In our case, we use Dask to handle this.\n",
    "\n",
    "(In general, if you are exclusively using Pandas, Modin might be easier, as it is drop-in compatible. But Dask can handle GeoDataFrames (unlike Modin), so we will use that here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13dfe321-0c78-4eb2-ae76-8ea409006306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast JSON library\n",
    "import ujson as json\n",
    "\n",
    "## Note for future self: If you need to muck around with JSON formatting, jq might be the way to go\n",
    "## Transforming to/from line-delimited, for example, is far simpler\n",
    "\n",
    "# Import the Dask libraries we need\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d789c42d-40e8-46ae-8eae-e4d8965afd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you try to load the BDD100K files directly into Dask, you'll run into some issues.\n",
    "# Dask Bags assume every line in a json file is a distinct json object.\n",
    "# So because BDD100K uses pretty json formatting, it will not load properly.\n",
    "# So we have to first remove all the newline characters in all of the files.\n",
    "\n",
    "# Note that the database size is huge, so make sure you have adequate disk space\n",
    "\n",
    "path = 'BDD100K/train/'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if ('json' in filename and not 'processed' in filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        f = open(filepath, 'r')\n",
    "        f = f.read().replace('\\n', '')\n",
    "        if 'gps' in f.read(): # Some of the files don't have GPS data.\n",
    "# These are unusuable to us, so we don't want them\n",
    "            if os.path.getsize(os.path.join(path,filename)) == 0: \n",
    "    # Fun fact: the BDD100K info dataset has a corrupted (empty) JSON file\n",
    "    # This caused an enormous headache on my end while debugging\n",
    "    # So now we will only process it if it's non-empty.\n",
    "    # Otherwise, we skip the processing part, but still delete the file\n",
    "    # (A more robust method would be to `try: json.loads`, but this would greatly increase processing time)\n",
    "                with open(path + 'processed-' + filename, 'w') as fp:\n",
    "                    print(f, file=fp)\n",
    "        # These files take up a lot of space on my harddrive, so I will remove them here.\n",
    "        # You may wish not to do this\n",
    "        os.remove(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "601745bd-8bb2-4dda-811b-4184f32c0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filename in os.listdir(path):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if 'ipynb' in filepath:\n",
    "            pass\n",
    "        else:\n",
    "            f = open(filepath, 'r')\n",
    "            if 'FeatureCollection' not in f.read(): # Some of the files don't have GPS data.\n",
    "    # These are unusuable to us, so we don't want them\n",
    "                os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "642634a5-3429-4a06-af2e-aaee4f6ca1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'BDD100K/train/'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "        if not 'postprocessed' in filename:\n",
    "            filepath = os.path.join(path, filename)\n",
    "            if 'ipynb' in filepath:\n",
    "                pass\n",
    "            else:\n",
    "                f = open(filepath, 'r')\n",
    "                if 'gps' not in f.read(): # Some of the files don't have GPS data.\n",
    "        # These are unusuable to us, so we don't want them\n",
    "        # In BDD100K, there's about 20000 files like this... quite unfortunate\n",
    "                    os.remove(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aef43fc-b67c-4970-a0fd-4a134e8505c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('No files found', 'BDD100K/train/processed-*.json')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now we load all the JSON files\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dfbag \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBDD100K/train/processed-*.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(json\u001b[38;5;241m.\u001b[39mloads)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This is a helper function for reformatting\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbdd_reformat\u001b[39m(jsonf):        \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/bag/text.py:154\u001b[0m, in \u001b[0;36mread_text\u001b[0;34m(urlpath, blocksize, compression, encoding, errors, linedelimiter, collection, storage_options, files_per_partition, include_path)\u001b[0m\n\u001b[1;32m    149\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    150\u001b[0m             delayed(attach_path)(entry, path) \u001b[38;5;28;01mfor\u001b[39;00m entry, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(blocks, paths)\n\u001b[1;32m    151\u001b[0m         ]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blocks:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files found\u001b[39m\u001b[38;5;124m\"\u001b[39m, urlpath)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collection:\n\u001b[1;32m    157\u001b[0m     blocks \u001b[38;5;241m=\u001b[39m from_delayed(blocks)\n",
      "\u001b[0;31mValueError\u001b[0m: ('No files found', 'BDD100K/train/processed-*.json')"
     ]
    }
   ],
   "source": [
    "# Now we load all the JSON files\n",
    "\n",
    "dfbag = db.read_text('BDD100K/train/processed-*.json').map(json.loads)\n",
    "\n",
    "# This is a helper function for reformatting\n",
    "\n",
    "def bdd_reformat(jsonf):        \n",
    "    listf = []\n",
    "    if jsonf.get('gps') != None:\n",
    "        for item in jsonf['gps']:\n",
    "            listf.append({\"type\": \"Feature\",\n",
    "          \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [item[\"longitude\"], item[\"latitude\"]]\n",
    "          },\n",
    "          \"properties\": {\n",
    "            \"timestamp\": item[\"timestamp\"],\n",
    "            \"altitude\": item[\"altitude\"],\n",
    "            \"speed\": item[\"speed\"],\n",
    "            \"vertical accuracy\": item[\"vertical accuracy\"],\n",
    "            \"horizontal accuracy\": item[\"horizontal accuracy\"]\n",
    "          }})\n",
    "        geojsonf = {\"type\": \"FeatureCollection\", \"features\": listf}\n",
    "        return geojsonf\n",
    "    else:\n",
    "        return jsonf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8ab87b-676f-466b-b5fa-394fbabae3b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfbag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m text_trap\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m### TODO: Consider outputting to better file format, e.g. Avro, Parquet\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mdfbag\u001b[49m\u001b[38;5;241m.\u001b[39mmap(bdd_reformat)\u001b[38;5;241m.\u001b[39mmap(json\u001b[38;5;241m.\u001b[39mdumps)\u001b[38;5;241m.\u001b[39mto_textfiles(path \u001b[38;5;241m+\u001b[39m files)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# now restore stdout function\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m__stdout__\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfbag' is not defined"
     ]
    }
   ],
   "source": [
    "path = 'BDD100K/train/'\n",
    "files = 'postprocessed-*.json'\n",
    "\n",
    "# create a text trap and redirect stdout\n",
    "text_trap = io.StringIO()\n",
    "sys.stdout = text_trap\n",
    "\n",
    "### TODO: Consider outputting to better file format, e.g. Avro, Parquet\n",
    "\n",
    "dfbag.map(bdd_reformat).map(json.dumps).to_textfiles(path + files)\n",
    "\n",
    "# now restore stdout function\n",
    "sys.stdout = sys.__stdout__\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if (not 'postprocessed' in filename):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        os.remove(filepath)\n",
    "\n",
    "# Note-- now may be a good time to zip and compress the processed files, in case something happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54edf3d1-5469-4a28-92e7-37d63dcb6726",
   "metadata": {},
   "source": [
    "Now our data has been post-processed to a format that is compatible with map matching algorithms. The simplest way to utilize the data is to load it all into a Dask Bag, and `take(n,npartitions=n)` as needed (alternatively, you can load each GeoJSON as a partition in a Dask GeoDataFrame-- but caution needs to be exercised here). However, if you wish to do more in-depth data analysis on the dataset, thousands of JSON files aren't exactly optimal. We could try to apply functions on Dask Bags, but the simpler solution is to store the files into a SQLite database. Then we can access the database as needed and access filtered data quickly. \n",
    "\n",
    "Note: if you have no interest in utilizing the GeoJSON structure, you should create a database from the unprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e71dd0f-aeb2-4ee9-aea7-ccc520a99913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case, it makes more sense to store it into a SQLite database, but MySQL, MariaDB, or other formats work perfectly well.\n",
    "\n",
    "# Fortunately, there are a lot of tools to convert GeoJSON to a spatially informed database\n",
    "# So instead of trying to do it ourselves, we will use an external tool to do the heavy lifting\n",
    "# Aren't you glad we processed the data into a more standard format?\n",
    "\n",
    "# Run this only once\n",
    "#! pip install geojson-to-sqlite\n",
    "#! sudo pamac install spatialite-gui # Optional, but improves our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffbb21-ea66-4573-beb7-91d9a99d5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'BDD100K/train/'\n",
    "\n",
    "# This for-loop takes a long time (maybe 24 hours?)\n",
    "# I don't know if you can parallelize creating tables in SQL, but if so, you probably should\n",
    "for filename in os.listdir(path):\n",
    "    ! geojson-to-sqlite BDD100K/postprocessed-BDD100K.db {filename[:-5]} {path+filename} --spatial-index --spatialite_mod=/usr/lib/mod_spatialite.so "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be659272-3925-4c07-b10f-a7e3edf777f0",
   "metadata": {},
   "source": [
    "In order to keep each track separate, we made them all individual tables. If you want them all in one table, simply change `{filename[:-5]}` to the name you would like.\n",
    "\n",
    "Let's run a query to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a011c-e2df-4a25-bdcb-999bd4897ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('BDD100K/postprocessed-BDD100K.db')\n",
    "conn.enable_load_extension(True)\n",
    "\n",
    "# Now we load spatialite\n",
    "conn.execute('SELECT load_extension(\"mod_spatialite\")')\n",
    "conn.execute('SELECT InitSpatialMetaData(1);')\n",
    "\n",
    "# libspatialite\n",
    "conn.execute('SELECT load_extension(\"libspatialite\")')\n",
    "conn.execute('SELECT InitSpatialMetaData();')\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute('SELECT ')\n",
    "\n",
    "#conn.commit()\n",
    "#conn.close()\n",
    "#del conn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c71060-7257-4b9f-b863-4e4e12377c4e",
   "metadata": {},
   "source": [
    "All done, right? Not quite. For example: is your data fused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f834059-1dba-458b-8413-c04ce7cb910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display data and see if fused\n",
    "\n",
    "# Note: If you chose to store all your data in one SQL Table,\n",
    "# Dask DataFrames can import from that.\n",
    "\n",
    "dfbag = db.read_text('BDD100K/train/postprocessed-*.json').map(json.loads)\n",
    "dfbag = dfbag.map(gpd.GeoDataFrame.from_features)\n",
    "dfbag.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ef7f7-b018-45ba-804c-71cf9b73da55",
   "metadata": {},
   "source": [
    "In our case, our data is already fused. But often you will have several datasets with asynchronous data that you will have to fuse first. We implemented a barebones method in mm_utils to handle this; here is an example of how to apply it.\n",
    "\n",
    "Note that your data needs to be a (Geo)DataFrame or GeoJSON. Also, the first column of all the datasets needs to be the time, and must all share the same time formatting. If you aren't sure your time format will work, we recommmend converting it all to Unix time (most languages have a built-in method to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055c433-404d-4db2-8161-24a5aa41b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create simulated asynchronous data\n",
    "\n",
    "# Get columns of data\n",
    "df = dfbag.take(1)[0]\n",
    "speed = origdf[['timestamp','speed']]\n",
    "\n",
    "# Create rng to create noisy data\n",
    "rng = np.random.default_rng()\n",
    "for i in speed.index[:-1]:\n",
    "    # Add a fake row consisting of a time between the timestamps, and a speed normally distributed around the average\n",
    "    fakerow = pd.DataFrame([[speed.iloc[i][0] + rng.random()*1000,(speed.iloc[i][1] + speed.iloc[i+1][1])/2 + np.random.normal(0, (np.abs(speed.iloc[i][1] - speed.iloc[i+1][1]))/4)]], columns = ['timestamp','speed'], index = [i+0.5])\n",
    "    speed = pd.concat([speed, fakerow])\n",
    "# Reset indices\n",
    "speed = speed.sort_index().reset_index(drop=True)\n",
    "speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798093f6-5f57-48e1-acd1-5ecc6ddf8140",
   "metadata": {},
   "source": [
    "Now we wish to fuse this DataFrame with our original dataframe (excluding the original speed column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e92f92-6eda-4879-bed7-02fc10d23f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = origdf[['geometry', 'timestamp', 'altitude', 'vertical accuracy', 'horizontal accuracy']]\n",
    "\n",
    "df_prox = mm_utils.fuse(df1,speed)\n",
    "df_avg = mm_utils.fuse(df1,speed)\n",
    "df_blah = mm_utils.fuse(df1,speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a0dfd-cdc8-420a-b9d0-86d194c52eec",
   "metadata": {},
   "source": [
    "Now let's see what the speed columns look like side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a13769-28da-4f75-8652-79458a0e4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up styling to facilitate comparison\n",
    "from IPython.display import display_html\n",
    "\n",
    "df_sty = df.loc['speed'].style.set_table_attributes(\"style='display:inline'\").set_caption('original df')\n",
    "df_prox_sty = df_prox.loc['speed'].style.set_table_attributes(\"style='display:inline'\").set_caption('proximity fuse')\n",
    "df_avg_sty = df_prox.loc['speed'].style.set_table_attributes(\"style='display:inline'\").set_caption('average fuse')\n",
    "df_blah_sty = df_blah.loc['speed'].style.set_table_attributes(\"style='display:inline'\").set_caption('df_blah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb80f6b-4a52-479b-b77c-edce68ec7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = \"\\xa0\" * 10\n",
    "display_html(df_sty._repr_html_() + space\n",
    "             + df_prox_sty._repr_html_() + space\n",
    "             + df_avg_sty._repr_html_() + space\n",
    "             + df_blah_sty._repr_html_(), raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
