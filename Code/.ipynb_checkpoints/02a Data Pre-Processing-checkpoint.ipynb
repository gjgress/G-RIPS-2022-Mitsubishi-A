{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2499d1-d466-47c7-bfba-e0605416a0f7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Is your data too messy to be utilized in 02? Look no further! This notebook walks through the data pre-processing methodology for our datasets, particularly BDD100K. We also include some helpful tips to make your data more compatible with these notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6a375a-1580-42e2-8906-e25c32d3a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjgress/.local/lib/python3.10/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox \n",
    "import time\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from algorithms import mm_utils\n",
    "\n",
    "%matplotlib inline\n",
    "#ox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428da85e-a982-4073-ac53-7f3ea62ea441",
   "metadata": {},
   "source": [
    "# Importing your Data\n",
    "\n",
    "If you are seriously testing your algorithm against data, chances are your dataset is huge. Blindly trying to import it into a Pandas (Geo)DataFrame is going to cause some issues, because it will attempt to load it all into memory (which is likely impossible).\n",
    "\n",
    "In our case, we use Dask to handle this.\n",
    "\n",
    "(In general, if you are exclusively using Pandas, Modin might be easier, as it is drop-in compatible. But Dask can handle GeoDataFrames (unlike Modin), so we will use that here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c69cf33-b528-42c6-85bf-22f435417402",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 38 (1924889989.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    listf.append({\"type\": \"Feature\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 38\n"
     ]
    }
   ],
   "source": [
    "# Fast JSON library\n",
    "import ujson as json\n",
    "# Import the Dask libraries we need\n",
    "import dask.bag as db\n",
    "\n",
    "# We load all the JSON files\n",
    "\n",
    "dfbag = db.read_text('BDD100K/train/*.json').map(json.loads)\n",
    "\n",
    "\n",
    "# Unfortunately, our JSON files aren't line-delimited, so I cannot increase the partition size.\n",
    "# As a result, parallelizing will be difficult (each partition is a file)\n",
    "# But once I do the conversion to the properly formatted JSON, I can ensure it is line-delimited\n",
    "# And so when I load it later for use, I will be able to parallelize\n",
    "\n",
    "# This is a helper function for reformatting\n",
    "\n",
    "def bdd_reformat(jsonf):        \n",
    "    listf = []\n",
    "    if jsonf.get('gps') != None:\n",
    "        for item in jsonf['gps']:\n",
    "            listf.append({\"type\": \"Feature\",\n",
    "          \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [item[\"longitude\"], item[\"latitude\"]]\n",
    "          },\n",
    "          \"properties\": {\n",
    "            \"timestamp\": item[\"timestamp\"],\n",
    "            \"altitude\": item[\"altitude\"],\n",
    "            \"speed\": item[\"speed\"],\n",
    "            \"vertical accuracy\": item[\"vertical accuracy\"],\n",
    "            \"horizontal accuracy\": item[\"horizontal accuracy\"]\n",
    "          }})\n",
    "        geojsonf = {\"type\": \"FeatureCollection\", \"features\": listf}\n",
    "        dfjson = json.dumps(geojsonf)\n",
    "        return dfjson\n",
    "    elif jsonf.get('locations') != None:\n",
    "        for item in jsonf['locations']:\n",
    "            listf.append({\"type\": \"Feature\",\n",
    "              \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [item[\"longitude\"], item[\"latitude\"]]\n",
    "              },\n",
    "              \"properties\": {\n",
    "                \"timestamp\": item[\"timestamp\"],\n",
    "                \"speed\": item[\"speed\"],\n",
    "                \"accuracy\": item[\"accuracy\"]\n",
    "              }})\n",
    "        geojsonf = {\"type\": \"FeatureCollection\", \"features\": listf}\n",
    "        dfjson = json.dumps(geojsonf)\n",
    "        return dfjson\n",
    "\n",
    "#        with open('BDD100K/train_clean/' + jsonf['rideID'][:8] + '-' + jsonf['filename'][:8] + '.json', 'w') as f:\n",
    "#            f.write(dfjson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8ab87b-676f-466b-b5fa-394fbabae3b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBDD100K/train/processed.*.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdfbag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbdd_reformat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_textfiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/bag/core.py:807\u001b[0m, in \u001b[0;36mBag.to_textfiles\u001b[0;34m(self, path, name_function, compression, encoding, compute, storage_options, last_endline, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(to_textfiles)\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_textfiles\u001b[39m(\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    806\u001b[0m ):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_textfiles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_endline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_endline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/bag/core.py:257\u001b[0m, in \u001b[0;36mto_textfiles\u001b[0;34m(b, path, name_function, compression, encoding, compute, storage_options, last_endline, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(b)(graph, name, b\u001b[38;5;241m.\u001b[39mnpartitions)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[0;32m--> 257\u001b[0m     \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/base.py:312\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    598\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 600\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/multiprocessing.py:232\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, num_workers, func_loads, func_dumps, optimize_graph, pool, initializer, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Note former versions used a multiprocessing Manager to share\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# a Queue between parent and workers, but this is fragile on Windows\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# (issue #1652).\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdsk3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_process_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdumps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/local.py:508\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m         _execute_task(task, data)  \u001b[38;5;66;03m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m         \u001b[43mraise_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m res, worker_id \u001b[38;5;241m=\u001b[39m loads(res_info)\n\u001b[1;32m    510\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m][key] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/local.py:315\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(exc, tb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m--> 315\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/local.py:221\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     task, data \u001b[38;5;241m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 221\u001b[0m     result \u001b[38;5;241m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m get_id()\n\u001b[1;32m    223\u001b[0m     result \u001b[38;5;241m=\u001b[39m dumps((result, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[38;5;241m=\u001b[39m arg[\u001b[38;5;241m0\u001b[39m], arg[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m(_execute_task(a, cache) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/bag/core.py:160\u001b[0m, in \u001b[0;36m_to_textfiles_chunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m     ensure \u001b[38;5;241m=\u001b[39m ensure_bytes\n\u001b[1;32m    159\u001b[0m started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m started:\n\u001b[1;32m    162\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(endline)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/bag/core.py:2026\u001b[0m, in \u001b[0;36m__next__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2026\u001b[0m         vals \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mnext\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miters]\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   2028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_all_iterators_consumed()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/bag/core.py:2026\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2026\u001b[0m         vals \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mnext\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miters]\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   2028\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_all_iterators_consumed()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/bag/core.py:2035\u001b[0m, in \u001b[0;36m__next__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2033\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwarg_keys, vals[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnkws :]))\n\u001b[1;32m   2034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(\u001b[38;5;241m*\u001b[39mvals)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "path = 'BDD100K/train/processed.*.json'\n",
    "\n",
    "dfbag.map(bdd_reformat).to_textfiles(path)\n",
    "\n",
    "#df = dd.read_json('BDD100K/train/*.json',orient = 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16c23e-a9af-4670-b225-b3cdb63ea1bf",
   "metadata": {},
   "source": [
    "Great, now we loaded our data into the notebook (you will have to repeat this process within other notebooks-- just copy the cell and put it at the start). But there's still a lot that needs to be done before we can plug in the data into our algorithms. For us, we need to reformat our data into a standard data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6723c8c3-986e-4cc8-bd5b-680b1020ff06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def bdd_reformat(dataframe):        \n",
    "    dataframe_new = []\n",
    "    if dataframe.index.isin(['gps']).any():\n",
    "        for item in dataframe.loc['gps'][0]:\n",
    "            dataframe_new.append({\"type\": \"Feature\",\n",
    "          \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [item[\"longitude\"], item[\"latitude\"]]\n",
    "          },\n",
    "          \"properties\": {\n",
    "            \"timestamp\": item[\"timestamp\"],\n",
    "            \"altitude\": item[\"altitude\"],\n",
    "            \"speed\": item[\"speed\"],\n",
    "            \"vertical accuracy\": item[\"vertical accuracy\"],\n",
    "            \"horizontal accuracy\": item[\"horizontal accuracy\"]\n",
    "          }})\n",
    "        dataframe_json = {\"type\": \"FeatureCollection\", \"features\": dataframe_new}\n",
    "        dfjson = json.dumps(dataframe_json)\n",
    "        with open('BDD100K/train_clean/' + dataframe.loc['rideID'][0][:8] + '-' + dataframe.loc['filename'][0][:8] + '.json', 'w') as f:\n",
    "            f.write(dfjson)\n",
    "\n",
    "# If you try to parallelize with Dask here, you'll run into some issues\n",
    "# Because there are thousands of partitions, Dask will attempt to schedule them all simultaneously\n",
    "# But Dask isn't built to schedule thousands of partitions (I think), so it will crash your kernel\n",
    "# Instead, we will use a more rudimentary parellelization process, and come back to Dask when the time is right\n",
    "Parallel(n_jobs=-1)(delayed(bdd_reformat)(df.partitions[i].compute()) for i in range(df.npartitions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588a4e9-2126-4852-ac5f-0be7304aaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now gdict is in GeoJSON format, so we will make it a JSON and export this to SQL database \n",
    "\n",
    "gjson = json.dumps(gdict)\n",
    "\n",
    "#For my testing only\n",
    "gdf = dd.read_json(gjson,orient = 'records', lines=True, blocksize=1000000) # If I did this right, I now have a Dask gdf with partition size of 1MB\n",
    "\n",
    "#with open('data.json', 'w') as f:\n",
    "#  f.write(geojsonfp)\n",
    "\n",
    "# Enable GeoJSON driver\n",
    "#fiona.drvsupport.supported_drivers[\"GeoJSON\"] = \"r\"\n",
    "\n",
    "#tripdata1 = gpd.read_file(\"data.json\", enabled_drivers=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6165885-780b-461b-b057-e344f1f5522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjgress/.local/lib/python3.10/site-packages/dask/dataframe/core.py:3088: FutureWarning: The frame.append method is deprecated and will be removed fromdask in a future version. Use dask.dataframe.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: concat, 4 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 type geometry properties\n",
       "npartitions=2                            \n",
       "               object   object    float64\n",
       "                  ...      ...        ...\n",
       "                  ...      ...        ...\n",
       "Dask Name: concat, 4 tasks"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf.append(gpd.GeoDataFrame(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147f5f9-2cf7-4c74-b96d-efe4a22f8637",
   "metadata": {},
   "source": [
    "That looks better. Let's export this to a database so we don't have to repeat this process every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71dd0f-aeb2-4ee9-aea7-ccc520a99913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case, it makes more sense to store it into a SQLite database\n",
    "# Then when we are ready to use it, we can load it smartly and call individual GeoDataFrames from the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c71060-7257-4b9f-b863-4e4e12377c4e",
   "metadata": {},
   "source": [
    "Much better. But there's still other things to check. For example: is your data fused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f834059-1dba-458b-8413-c04ce7cb910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display data and see if fused\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "BDD100K_train = create_engine('sqlite:///BDD100K_train.db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ef7f7-b018-45ba-804c-71cf9b73da55",
   "metadata": {},
   "source": [
    "In our case, our data is already fused. But often you will have several datasets with asynchronous data that you will have to fuse first. We implemented a barebones method in mm_utils to handle this; here is an example of how to apply it.\n",
    "\n",
    "Note that your data needs to be a (Geo)DataFrame. Also, the first column of all the datasets needs to be the time, and must all share the same time formatting. If you aren't sure your time format will work, we recommmend converting it all to Unix time (most languages have a built-in method to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055c433-404d-4db2-8161-24a5aa41b37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95be3d9e-5d87-403a-9c5d-c79c4de82928",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
