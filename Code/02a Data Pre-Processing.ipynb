{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2499d1-d466-47c7-bfba-e0605416a0f7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Is your data too messy to be utilized in 02? Look no further! This notebook walks through the data pre-processing methodology for our datasets, particularly BDD100K. We also include some helpful tips to make your data more compatible with these notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a6a375a-1580-42e2-8906-e25c32d3a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox \n",
    "import time\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from algorithms import mm_utils\n",
    "\n",
    "%matplotlib inline\n",
    "#ox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428da85e-a982-4073-ac53-7f3ea62ea441",
   "metadata": {},
   "source": [
    "# Importing your Data\n",
    "\n",
    "If you are seriously testing your algorithm against data, chances are your dataset is huge. Blindly trying to import it into a Pandas (Geo)DataFrame is going to cause some issues, because it will attempt to load it all into memory (which is likely impossible).\n",
    "\n",
    "In our case, we use Dask to handle this.\n",
    "\n",
    "(In general, if you are exclusively using Pandas, Modin might be easier, as it is drop-in compatible. But Dask can handle GeoDataFrames (unlike Modin), so we will use that here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75bbae47-b0d2-4dfd-9117-986b3749d60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rideID</th>\n",
       "      <td>0a006b7b99b3d335d0e5371422d15482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accelerometer</th>\n",
       "      <td>[{'y': -0.011, 'timestamp': 1504332296645, 'z'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyro</th>\n",
       "      <td>[{'y': 0.18710000000000002, 'timestamp': 15043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timelapse</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locations</th>\n",
       "      <td>[{'timestamp': 1504332296000, 'longitude': -73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <td>3e22a8a2-ade4-4516-a901-f3e2524fc4d9.mov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startTime</th>\n",
       "      <td>1504332296633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endTime</th>\n",
       "      <td>1504332336703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>fb468f0e9d117bbf76965c6df604926b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gps</th>\n",
       "      <td>[{'timestamp': 1504332297000, 'altitude': 8.57...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               0\n",
       "rideID                          0a006b7b99b3d335d0e5371422d15482\n",
       "accelerometer  [{'y': -0.011, 'timestamp': 1504332296645, 'z'...\n",
       "gyro           [{'y': 0.18710000000000002, 'timestamp': 15043...\n",
       "timelapse                                                  False\n",
       "locations      [{'timestamp': 1504332296000, 'longitude': -73...\n",
       "filename                3e22a8a2-ade4-4516-a901-f3e2524fc4d9.mov\n",
       "startTime                                          1504332296633\n",
       "endTime                                            1504332336703\n",
       "id                              fb468f0e9d117bbf76965c6df604926b\n",
       "gps            [{'timestamp': 1504332297000, 'altitude': 8.57..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Dask libraries we need\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "\n",
    "# We load all the JSON files\n",
    "\n",
    "df = dd.read_json('BDD100K/train/*.json',orient = 'index')\n",
    "#df = dd.read_json('BDD100K/train/*.json')\n",
    "df.partitions[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16c23e-a9af-4670-b225-b3cdb63ea1bf",
   "metadata": {},
   "source": [
    "Great, now we loaded our data into the notebook (you will have to repeat this process within other notebooks-- just copy the cell and put it at the start). But there's still a lot that needs to be done before we can plug in the data into our algorithms. For us, we need to reformat our data into a standard data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11fd838c-28a6-42f2-9f17-c0461f8951f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'gps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m geodf \u001b[38;5;241m=\u001b[39m dgpd\u001b[38;5;241m.\u001b[39mfrom_geopandas(pd\u001b[38;5;241m.\u001b[39mDataFrame([]),npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgps\u001b[49m:\n\u001b[1;32m      4\u001b[0m     temp \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizontal accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizontal accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m   }}\n\u001b[1;32m     16\u001b[0m     geodf\u001b[38;5;241m.\u001b[39mappend(temp)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'gps'"
     ]
    }
   ],
   "source": [
    "geodf = dgpd.from_geopandas(pd.DataFrame([]),npartitions=1)\n",
    "\n",
    "## TODO: Parallelize the following loop\n",
    "## Also, maybe don't write to JSON? Whatever format works best for SQLite database\n",
    "\n",
    "for row in df.partitions[0].compute().loc['gps'][0]:\n",
    "    temp = {\"type\": \"Feature\",\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Point\",\n",
    "    \"coordinates\": [row[\"longitude\"], row[\"latitude\"]]\n",
    "  },\n",
    "  \"properties\": {\n",
    "    \"timestamp\": row[\"timestamp\"],\n",
    "    \"altitude\": row[\"altitude\"],\n",
    "    \"speed\": row[\"speed\"],\n",
    "    \"vertical accuracy\": row[\"vertical accuracy\"],\n",
    "    \"horizontal accuracy\": row[\"horizontal accuracy\"]\n",
    "  }}\n",
    "    geodf.append(temp)\n",
    "\n",
    "#geofp = {\"type\": \"FeatureCollection\", \"features\": geofp}\n",
    "        \n",
    "#geojsonfp = json.dumps(geofp)\n",
    "#with open('data.json', 'w') as f:\n",
    "#  f.write(geojsonfp)\n",
    "\n",
    "# Enable GeoJSON driver\n",
    "#fiona.drvsupport.supported_drivers[\"GeoJSON\"] = \"r\"\n",
    "\n",
    "#tripdata1 = gpd.read_file(\"data.json\", enabled_drivers=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147f5f9-2cf7-4c74-b96d-efe4a22f8637",
   "metadata": {},
   "source": [
    "That looks better. Let's export this to a database so we don't have to repeat this process every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71dd0f-aeb2-4ee9-aea7-ccc520a99913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case, it makes more sense to store it into a SQLite database\n",
    "# Then when we are ready to use it, we can load it smartly and call individual GeoDataFrames from the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c71060-7257-4b9f-b863-4e4e12377c4e",
   "metadata": {},
   "source": [
    "Much better. But there's still other things to check. For example: is your data fused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f834059-1dba-458b-8413-c04ce7cb910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display data and see if fused\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "BDD100K_train = create_engine('sqlite:///BDD100K_train.db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ef7f7-b018-45ba-804c-71cf9b73da55",
   "metadata": {},
   "source": [
    "In our case, our data is already fused. But often you will have several datasets with asynchronous data that you will have to fuse first. We implemented a barebones method in mm_utils to handle this; here is an example of how to apply it.\n",
    "\n",
    "Note that your data needs to be a (Geo)DataFrame. Also, the first column of all the datasets needs to be the time, and must all share the same time formatting. If you aren't sure your time format will work, we recommmend converting it all to Unix time (most languages have a built-in method to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055c433-404d-4db2-8161-24a5aa41b37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95be3d9e-5d87-403a-9c5d-c79c4de82928",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
