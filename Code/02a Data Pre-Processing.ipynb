{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2499d1-d466-47c7-bfba-e0605416a0f7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Is your data too messy to be utilized in 02? Look no further! This notebook walks through the data pre-processing methodology for our datasets, particularly BDD100K. We also include some helpful tips to make your data more compatible with these notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6a375a-1580-42e2-8906-e25c32d3a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjgress/.local/lib/python3.10/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox \n",
    "import time\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from algorithms import mm_utils\n",
    "\n",
    "%matplotlib inline\n",
    "#ox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428da85e-a982-4073-ac53-7f3ea62ea441",
   "metadata": {},
   "source": [
    "# Importing your Data\n",
    "\n",
    "If you are seriously testing your algorithm against data, chances are your dataset is huge. Blindly trying to import it into a Pandas (Geo)DataFrame is going to cause some issues, because it will attempt to load it all into memory (which is likely impossible).\n",
    "\n",
    "In our case, we use Dask to handle this.\n",
    "\n",
    "(In general, if you are exclusively using Pandas, Modin might be easier, as it is drop-in compatible. But Dask can handle GeoDataFrames (unlike Modin), so we will use that here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bbae47-b0d2-4dfd-9117-986b3749d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Dask libraries we need\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "\n",
    "# We load all the JSON files\n",
    "\n",
    "# Unfortunately, our JSON files aren't line-delimited, so I cannot increase partition size.\n",
    "# As a result, parallelizing will be difficult (each partition is a file)\n",
    "# But once I do the conversion to the properly formatted JSON, I can ensure it is line-delimited\n",
    "# And so when I load it later for use, I will be able to parallelize\n",
    "\n",
    "df = dd.read_json('BDD100K/train/*.json',orient = 'index')\n",
    "#df.partitions[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16c23e-a9af-4670-b225-b3cdb63ea1bf",
   "metadata": {},
   "source": [
    "Great, now we loaded our data into the notebook (you will have to repeat this process within other notebooks-- just copy the cell and put it at the start). But there's still a lot that needs to be done before we can plug in the data into our algorithms. For us, we need to reformat our data into a standard data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723c8c3-986e-4cc8-bd5b-680b1020ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = dgpd.from_geopandas(pd.DataFrame([]),npartitions=1)\n",
    "\n",
    "## TODO: Parallelize the following loop\n",
    "## Also, maybe don't write to JSON? Whatever format works best for SQLite database\n",
    "\n",
    "for part in df.partitions:\n",
    "    row = part.compute().loc['gps'][0]\n",
    "    temp = gpd.GeoDataFrame({\"type\": \"Feature\",\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Point\",\n",
    "    \"coordinates\": [row[\"longitude\"], row[\"latitude\"]]\n",
    "  },\n",
    "  \"properties\": {\n",
    "    \"timestamp\": row[\"timestamp\"],\n",
    "    \"altitude\": row[\"altitude\"],\n",
    "    \"speed\": row[\"speed\"],\n",
    "    \"vertical accuracy\": row[\"vertical accuracy\"],\n",
    "    \"horizontal accuracy\": row[\"horizontal accuracy\"]\n",
    "  }})\n",
    "    gdf.append(temp)\n",
    "\n",
    "gdict = {\"type\": \"FeatureCollection\", \"features\": gdf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588a4e9-2126-4852-ac5f-0be7304aaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now gdict is in GeoJSON format, so we will make it a JSON and export this to SQL database \n",
    "\n",
    "gjson = json.dumps(gdict)\n",
    "\n",
    "#For my testing only\n",
    "gdf = dd.read_json(gjson,orient = 'records', lines=True, blocksize=1000000) # If I did this right, I now have a Dask gdf with partition size of 1MB\n",
    "\n",
    "#with open('data.json', 'w') as f:\n",
    "#  f.write(geojsonfp)\n",
    "\n",
    "# Enable GeoJSON driver\n",
    "#fiona.drvsupport.supported_drivers[\"GeoJSON\"] = \"r\"\n",
    "\n",
    "#tripdata1 = gpd.read_file(\"data.json\", enabled_drivers=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6165885-780b-461b-b057-e344f1f5522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjgress/.local/lib/python3.10/site-packages/dask/dataframe/core.py:3088: FutureWarning: The frame.append method is deprecated and will be removed fromdask in a future version. Use dask.dataframe.concat instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: concat, 4 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 type geometry properties\n",
       "npartitions=2                            \n",
       "               object   object    float64\n",
       "                  ...      ...        ...\n",
       "                  ...      ...        ...\n",
       "Dask Name: concat, 4 tasks"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodf.append(gpd.GeoDataFrame(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147f5f9-2cf7-4c74-b96d-efe4a22f8637",
   "metadata": {},
   "source": [
    "That looks better. Let's export this to a database so we don't have to repeat this process every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71dd0f-aeb2-4ee9-aea7-ccc520a99913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case, it makes more sense to store it into a SQLite database\n",
    "# Then when we are ready to use it, we can load it smartly and call individual GeoDataFrames from the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c71060-7257-4b9f-b863-4e4e12377c4e",
   "metadata": {},
   "source": [
    "Much better. But there's still other things to check. For example: is your data fused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f834059-1dba-458b-8413-c04ce7cb910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display data and see if fused\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "BDD100K_train = create_engine('sqlite:///BDD100K_train.db')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ef7f7-b018-45ba-804c-71cf9b73da55",
   "metadata": {},
   "source": [
    "In our case, our data is already fused. But often you will have several datasets with asynchronous data that you will have to fuse first. We implemented a barebones method in mm_utils to handle this; here is an example of how to apply it.\n",
    "\n",
    "Note that your data needs to be a (Geo)DataFrame. Also, the first column of all the datasets needs to be the time, and must all share the same time formatting. If you aren't sure your time format will work, we recommmend converting it all to Unix time (most languages have a built-in method to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055c433-404d-4db2-8161-24a5aa41b37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95be3d9e-5d87-403a-9c5d-c79c4de82928",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
